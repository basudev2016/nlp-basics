{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "70ab2642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import contractions\n",
    "import pickle\n",
    "from nltk.tokenize.treebank import TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "12133480",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download NLTK resources (only once needed)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dfd9a6de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text\n",
      "0   Hey THERE!!! How's everything going? 😃👍 #excited\n",
      "1  I'm loving this new phone... It's AMAZING!!! P...\n",
      "2  She said, 'I'll be there at 5pm!!' But she arr...\n",
      "3  Let's test NLP's ability to clean texts: email...\n",
      "4  Best day everrrrr!!! Gonna remember it forever...\n",
      "5  Can't believe it's already 2025... Time flies! 🕒🚀\n",
      "6         Ewwww, that was soooo gross 🤢🤮 #neveragain\n",
      "7  Visit us at www.awesome-place.com or call 1800...\n",
      "8  OMG!!! 😱😱 Such an unexpected turn of events......\n",
      "9  Happy birthdayyyyyyy!!!! 🎂🎉🎈 Wishing you lots ...\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load the messy data\n",
    "# Read the CSV with UTF-8 encoding\n",
    "df = pd.read_csv('D:/Nitte_NLP/nlp-basics/Day-1/2_Text-preprocessong/text_preprocessing.csv', encoding='utf-8')\n",
    "\n",
    "# Show the first 5 rows\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "27d9596a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_lower</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hey THERE!!! How's everything going? 😃👍 #excited</td>\n",
       "      <td>hey there!!! how's everything going? 😃👍 #excited</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I'm loving this new phone... It's AMAZING!!! P...</td>\n",
       "      <td>i'm loving this new phone... it's amazing!!! p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>She said, 'I'll be there at 5pm!!' But she arr...</td>\n",
       "      <td>she said, 'i'll be there at 5pm!!' but she arr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Let's test NLP's ability to clean texts: email...</td>\n",
       "      <td>let's test nlp's ability to clean texts: email...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Best day everrrrr!!! Gonna remember it forever...</td>\n",
       "      <td>best day everrrrr!!! gonna remember it forever...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Can't believe it's already 2025... Time flies! 🕒🚀</td>\n",
       "      <td>can't believe it's already 2025... time flies! 🕒🚀</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Ewwww, that was soooo gross 🤢🤮 #neveragain</td>\n",
       "      <td>ewwww, that was soooo gross 🤢🤮 #neveragain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Visit us at www.awesome-place.com or call 1800...</td>\n",
       "      <td>visit us at www.awesome-place.com or call 1800...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>OMG!!! 😱😱 Such an unexpected turn of events......</td>\n",
       "      <td>omg!!! 😱😱 such an unexpected turn of events......</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Happy birthdayyyyyyy!!!! 🎂🎉🎈 Wishing you lots ...</td>\n",
       "      <td>happy birthdayyyyyyy!!!! 🎂🎉🎈 wishing you lots ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0   Hey THERE!!! How's everything going? 😃👍 #excited   \n",
       "1  I'm loving this new phone... It's AMAZING!!! P...   \n",
       "2  She said, 'I'll be there at 5pm!!' But she arr...   \n",
       "3  Let's test NLP's ability to clean texts: email...   \n",
       "4  Best day everrrrr!!! Gonna remember it forever...   \n",
       "5  Can't believe it's already 2025... Time flies! 🕒🚀   \n",
       "6         Ewwww, that was soooo gross 🤢🤮 #neveragain   \n",
       "7  Visit us at www.awesome-place.com or call 1800...   \n",
       "8  OMG!!! 😱😱 Such an unexpected turn of events......   \n",
       "9  Happy birthdayyyyyyy!!!! 🎂🎉🎈 Wishing you lots ...   \n",
       "\n",
       "                                          text_lower  \n",
       "0   hey there!!! how's everything going? 😃👍 #excited  \n",
       "1  i'm loving this new phone... it's amazing!!! p...  \n",
       "2  she said, 'i'll be there at 5pm!!' but she arr...  \n",
       "3  let's test nlp's ability to clean texts: email...  \n",
       "4  best day everrrrr!!! gonna remember it forever...  \n",
       "5  can't believe it's already 2025... time flies! 🕒🚀  \n",
       "6         ewwww, that was soooo gross 🤢🤮 #neveragain  \n",
       "7  visit us at www.awesome-place.com or call 1800...  \n",
       "8  omg!!! 😱😱 such an unexpected turn of events......  \n",
       "9  happy birthdayyyyyyy!!!! 🎂🎉🎈 wishing you lots ...  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 2: Lowercasing\n",
    "df['text_lower'] = df['text'].str.lower()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bcc8046a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After Expanding Contractions:\n",
      "0    hey there!!! how is everything going? 😃👍 #excited\n",
      "1    i am loving this new phone... it is amazing!!!...\n",
      "2    she said, 'i will be there at 5pm!!' but she a...\n",
      "3    let us test nlp's ability to clean texts: emai...\n",
      "4    best day everrrrr!!! going to remember it fore...\n",
      "Name: text_no_contractions, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Expand contractions\n",
    "df['text_no_contractions'] = df['text_lower'].apply(lambda x: contractions.fix(x))\n",
    "print(\"\\nAfter Expanding Contractions:\")\n",
    "print(df['text_no_contractions'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "93eb1b67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After Removing Punctuation:\n",
      "0         hey there how is everything going 😃👍 excited\n",
      "1    i am loving this new phone it is amazing price...\n",
      "2    she said i will be there at 5pm but she arrive...\n",
      "3    let us test nlps ability to clean texts emails...\n",
      "4    best day everrrrr going to remember it forever...\n",
      "Name: text_no_punct, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Remove punctuation\n",
    "df['text_no_punct'] = df['text_no_contractions'].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))\n",
    "print(\"\\nAfter Removing Punctuation:\")\n",
    "print(df['text_no_punct'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b7dd886b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After Removing Digits:\n",
      "0         hey there how is everything going 😃👍 excited\n",
      "1    i am loving this new phone it is amazing price  🤳\n",
      "2    she said i will be there at pm but she arrived...\n",
      "3    let us test nlps ability to clean texts emails...\n",
      "4    best day everrrrr going to remember it forever...\n",
      "Name: text_no_digits, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Remove digits\n",
    "df['text_no_digits'] = df['text_no_punct'].apply(lambda x: re.sub(r'\\d+', '', x))\n",
    "print(\"\\nAfter Removing Digits:\")\n",
    "print(df['text_no_digits'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6ee46eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Remove emojis and URL \n",
    "# 1. Emoji pattern (expanded correctly including all missing blocks)\n",
    "emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  \n",
    "                           u\"\\U0001F300-\\U0001F5FF\"\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"\n",
    "                           u\"\\U00002700-\\U000027BF\"\n",
    "                           u\"\\U0001F900-\\U0001F9FF\"\n",
    "                           u\"\\U0001FA70-\\U0001FAFF\"\n",
    "                           u\"\\U00002600-\\U000026FF\"\n",
    "                           u\"\\U00002300-\\U000023FF\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "\n",
    "# 2. URL pattern\n",
    "url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "\n",
    "# Step 1: Remove URLs first\n",
    "df['text_no_urls'] = df['text_no_digits'].apply(lambda x: url_pattern.sub(r'', x))\n",
    "\n",
    "# Step 2: Then remove emojis\n",
    "df['text_no_emojis'] = df['text_no_urls'].apply(lambda x: emoji_pattern.sub(r'', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6f693cd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After Word Tokenization:\n",
      "0    [hey, there, how, is, everything, going, excited]\n",
      "1    [i, am, loving, this, new, phone, it, is, amaz...\n",
      "2    [she, said, i, will, be, there, at, pm, but, s...\n",
      "3    [let, us, test, nlps, ability, to, clean, text...\n",
      "4    [best, day, everrrrr, going, to, remember, it,...\n",
      "Name: word_tokens, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Tokenization (Word-level)\n",
    "# (Optional) You can skip this if you already know stopwords are needed later\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Manually load 'punkt' tokenizer (optional, only for sentence tokenization if needed)\n",
    "# PUNKT_PATH = r'C:\\Users\\Admin\\AppData\\Roaming\\nltk_data\\tokenizers\\punkt\\english.pickle'\n",
    "# with open(PUNKT_PATH, 'rb') as f:\n",
    "#     sent_tokenizer = pickle.load(f)\n",
    "\n",
    "# Initialize Treebank Word Tokenizer\n",
    "word_tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "# Perform word tokenization\n",
    "df['word_tokens'] = df['text_no_emojis'].apply(lambda x: word_tokenizer.tokenize(x))\n",
    "\n",
    "print(\"\\nAfter Word Tokenization:\")\n",
    "print(df['word_tokens'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ac58d575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After Stopword Removal:\n",
      "0                    [hey, everything, going, excited]\n",
      "1                 [loving, new, phone, amazing, price]\n",
      "2                                  [said, pm, arrived]\n",
      "3    [let, us, test, nlps, ability, clean, texts, e...\n",
      "4    [best, day, everrrrr, going, remember, forever...\n",
      "Name: tokens_no_stopwords, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Step 8: Stopword Removal\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df['tokens_no_stopwords'] = df['word_tokens'].apply(lambda tokens: [w for w in tokens if w not in stop_words])\n",
    "print(\"\\nAfter Stopword Removal:\")\n",
    "print(df['tokens_no_stopwords'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7e3d4803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After Generating Bigrams:\n",
      "0    [(hey, everything), (everything, going), (goin...\n",
      "1    [(loving, new), (new, phone), (phone, amazing)...\n",
      "2                          [(said, pm), (pm, arrived)]\n",
      "3    [(let, us), (us, test), (test, nlps), (nlps, a...\n",
      "4    [(best, day), (day, everrrrr), (everrrrr, goin...\n",
      "Name: bigrams, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Step 9: N-grams (bigram example)\n",
    "from nltk import ngrams\n",
    "\n",
    "def generate_bigrams(tokens):\n",
    "    return list(ngrams(tokens, 2))\n",
    "\n",
    "df['bigrams'] = df['tokens_no_stopwords'].apply(generate_bigrams)\n",
    "print(\"\\nAfter Generating Bigrams:\")\n",
    "print(df['bigrams'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "286b6f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After Lemmatization:\n",
      "0                    [hey, everything, going, excited]\n",
      "1                 [loving, new, phone, amazing, price]\n",
      "2                                  [said, pm, arrived]\n",
      "3    [let, u, test, nlp, ability, clean, text, emai...\n",
      "4    [best, day, everrrrr, going, remember, forever...\n",
      "Name: lemmatized_tokens, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Step 10: Lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_tokens(tokens):\n",
    "    return [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "df['lemmatized_tokens'] = df['tokens_no_stopwords'].apply(lemmatize_tokens)\n",
    "print(\"\\nAfter Lemmatization:\")\n",
    "print(df['lemmatized_tokens'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e53bce93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After Stemming:\n",
      "0                            [hey, everyth, go, excit]\n",
      "1                      [love, new, phone, amaz, price]\n",
      "2                                    [said, pm, arriv]\n",
      "3    [let, us, test, nlp, abil, clean, text, email,...\n",
      "4    [best, day, everrrrr, go, rememb, forev, bless...\n",
      "Name: stemmed_tokens, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Step 11: Stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def stem_tokens(tokens):\n",
    "    return [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "df['stemmed_tokens'] = df['tokens_no_stopwords'].apply(stem_tokens)\n",
    "print(\"\\nAfter Stemming:\")\n",
    "print(df['stemmed_tokens'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d3ba0b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('D:/Nitte_NLP/nlp-basics/Day-1/2_Text-preprocessong/cleaned_text.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d53d8c7",
   "metadata": {},
   "source": [
    "Assignment - What is the difference between stemming and Lemmatization. where each should be applied?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19de5246",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
